{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fecacc2-813f-4865-aa78-a562a3681a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Window Functions\n",
    "###  Basic Window Functions\n",
    "####  Use orderBy() when order matters:\n",
    " ● Ranking Functions (row_number, rank, dense_rank)\n",
    " ● Offset Functions (lead, lag)\n",
    " ● Cumulative Aggregations (sum, avg with rowsBetween)\n",
    "####  Skip orderBy() when order is irrelevant:\n",
    " ● Partition-wise Aggregates (sum, avg, count)\n",
    " ● Row-Agnostic Aggregations (max, min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37702e1f-c46c-4d28-9aca-9f24a031dabd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(employee_name=\"Alice\", employee_id=101, department=\"HR\", salary=55000),\n",
    "    Row(employee_name=\"Bob\", employee_id=102, department=\"Finance\", salary=72000),\n",
    "    Row(employee_name=\"Charlie\", employee_id=103, department=\"IT\", salary=95000),\n",
    "    Row(employee_name=\"David\", employee_id=104, department=\"HR\", salary=60000),\n",
    "    Row(employee_name=\"Eva\", employee_id=105, department=\"Finance\", salary=80000),\n",
    "    Row(employee_name=\"Frank\", employee_id=106, department=\"IT\", salary=120000),\n",
    "    Row(employee_name=\"Grace\", employee_id=107, department=\"Marketing\", salary=65000),\n",
    "    Row(employee_name=\"Helen\", employee_id=108, department=\"Marketing\", salary=70000),\n",
    "    Row(employee_name=\"Ian\", employee_id=109, department=\"IT\", salary=105000),\n",
    "    Row(employee_name=\"Jane\", employee_id=110, department=\"HR\", salary=58000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da93a8c5-745c-4de5-abca-8c3f8c833b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, rank, dense_rank, lag, lead, sum, avg\n",
    "# Define window specification (partition by department, order by salary descending)\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply window functions\n",
    "#**row_number:** Assigns unique numbers to each row in a partition.\n",
    "df = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "# **rank:** Similar to row_number but allows rank gaps.\n",
    "df = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "# **dense_rank:** Like rank but without gaps.\n",
    "df = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "# **lag:** Gets the previous row's value.\n",
    "df = df.withColumn(\"previous_salary\", lag(\"salary\").over(window_spec))\n",
    "# **lead:** Gets the next row's value.\n",
    "df = df.withColumn(\"next_salary\", lead(\"salary\").over(window_spec))\n",
    "# **sum:** Computes a running total.\n",
    "df = df.withColumn(\"running_total\", sum(\"salary\").over(window_spec))\n",
    "# **avg:** Computes a moving average.\n",
    "df = df.withColumn(\"moving_avg\", avg(\"salary\").over(window_spec))\n",
    "# Show result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce2b809-ee9b-47b9-babf-93abd977c0ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### With Rows Between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcf7b28-6d2c-48fa-b06b-3aca297c44da",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766956689844}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum, avg, min, max, count\n",
    "#1. Rolling sum over the last 2 rows and current row\n",
    "window_spec1 = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(-2, 0)\n",
    "df = df.withColumn(\"rolling_sum_last_2\", sum(\"salary\").over(window_spec1))\n",
    "#2. Moving average including previous, current, and next row\n",
    "window_spec2 = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(-1, 1)\n",
    "df = df.withColumn(\"moving_avg2\", avg(\"salary\").over(window_spec2))\n",
    "\n",
    "#3. Rolling minimum for current and next 2 rows\n",
    "window_spec3 = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(0, 2)\n",
    "df = df.withColumn(\"rolling_min_next_2\", min(\"salary\").over(window_spec3))\n",
    "#4. Maximum salary over all previous rows (running max)\n",
    "window_spec4 = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "df = df.withColumn(\"running_max\", max(\"salary\").over(window_spec4))\n",
    "#5. Count total rows within the window (entire partition)\n",
    "window_spec5 = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "df = df.withColumn(\"total_rows\", count(\"salary\").over(window_spec5))\n",
    "# Show result\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8010a497-9d0b-4931-aa14-32ce0f469a49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-windowsing-functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
