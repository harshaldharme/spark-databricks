{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5cf6fa-8805-4982-8f79-ad4b2f3e9eaf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766959923855}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(employee_id=1, name=\"Alice\", salary=70000, department=\"HR\"),\n",
    "    Row(employee_id=2, name=\"Bob\", salary=80000, department=\"Engineering\"),\n",
    "    Row(employee_id=3, name=\"Charlie\", salary=75000, department=\"HR\"),\n",
    "    Row(employee_id=4, name=\"David\", salary=90000, department=\"Engineering\"),\n",
    "    Row(employee_id=5, name=\"Eve\", salary=65000, department=\"Finance\"),\n",
    "    Row(employee_id=6, name=\"Frank\", salary=72000, department=\"Finance\"),\n",
    "    Row(employee_id=7, name=\"Grace\", salary=85000, department=\"Engineering\"),\n",
    "    Row(employee_id=8, name=\"Heidi\", salary=68000, department=\"HR\"),\n",
    "    Row(employee_id=9, name=\"Ivan\", salary=95000, department=\"Engineering\"),\n",
    "    Row(employee_id=10, name=\"Judy\", salary=70000, department=\"Finance\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb62e780-01e4-4c6e-b97a-4fb65ba49fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Running SQL Queries\n",
    "### With Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941d952e-bfa9-48b8-9d31-288c900c5fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a temporary SQL table from a DataFrame\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "# Select all columns\n",
    "df_sql = spark.sql(\"SELECT * FROM employees\")\n",
    "\n",
    "# Select specific columns\n",
    "df_sql = spark.sql(\"SELECT name, salary FROM employees\")\n",
    "# Filter data\n",
    "df_sql = spark.sql(\"SELECT * FROM employees WHERE salary > 50000\")\n",
    "# Aggregations\n",
    "df_sql = spark.sql(\"SELECT department, AVG(salary) AS avg_salary FROM employees GROUP BY department\")\n",
    "# Sorting\n",
    "df_sql = spark.sql(\"SELECT * FROM employees ORDER BY salary DESC\")\n",
    "# Using LIMIT\n",
    "df_sql = spark.sql(\"SELECT * FROM employees LIMIT 10\")\n",
    "# Using CASE WHEN\n",
    "df_sql = spark.sql(\"\"\"\n",
    "SELECT name, salary,\n",
    "CASE\n",
    "WHEN salary > 85000 THEN 'High'\n",
    "ELSE 'Low'\n",
    "END AS salary_category\n",
    "FROM employees\n",
    "\"\"\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850c672a-1e9b-4674-878c-8ec2ff1a3839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Without Temp View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a06e1a-3f63-42e4-a0d0-50f3efbdc5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use Spark SQL with a variable and pass the dataframe\n",
    "spark.sql(\"select * from {customers_df}\",customers_df = df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4f8640-70f3-4137-94b2-4753ba264a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-sql-operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
