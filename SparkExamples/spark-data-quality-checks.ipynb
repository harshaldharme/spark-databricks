{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad5aa9c-a661-43c3-bc45-475f903bbaf9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766962226694}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"DataQualityAudit\").getOrCreate()\n",
    "\n",
    "# --- 1. SETUP: Create Sample Data with Anomalies ---\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"cust_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"updated_at\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Data containing all 7 issues:\n",
    "# Duplicates, NULLs, Outliers, Bad Patterns, Stale data, and Missing Parents\n",
    "data = [\n",
    "    (\"ORD01\", \"C1\", 100.0, \"a@b.com\", datetime.now()),\n",
    "    (\"ORD01\", \"C1\", 100.0, \"a@b.com\", datetime.now()),            # 1. Duplicate\n",
    "    (\"ORD02\", None, 50.0, \"c@d.com\", datetime.now()),             # 2. NULL (Critical)\n",
    "    (\"ORD03\", \"C2\", -50.0, \"e@f.com\", datetime.now()),            # 3. Numeric Outlier (Negative)\n",
    "    (\"ORD04\", \"C3\", 75.0, \"invalid_email\", datetime.now()),       # 4. Pattern Mismatch\n",
    "    (\"ORD05\", \"C1\", 20.0, \"g@h.com\", datetime.now() - timedelta(days=5)), # 5. Stale (Freshness)\n",
    "    (\"ORD06\", \"C99\", 30.0, \"i@j.com\", datetime.now()),            # 6. Integrity (C99 doesn't exist)\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()\n",
    "\n",
    "# Parent Table for Referential Integrity\n",
    "df_parents = spark.createDataFrame([(\"C1\",), (\"C2\",), (\"C3\",)], [\"cust_id\"])\n",
    "df_parents.display()\n",
    "\n",
    "# --- 2. THE 7 QUALITY CHECKS & FIXES ---\n",
    "\n",
    "print(\"Starting Quality Audit...\")\n",
    "\n",
    "# 1. Freshness Check (Threshold: 24h)\n",
    "limit = datetime.now() - timedelta(hours=24)\n",
    "stale = df.filter(F.col(\"updated_at\") < limit)\n",
    "print(f\"Check 1 (Freshness): {stale.count()} stale rows found.\")\n",
    "df_clean = df.filter(F.col(\"updated_at\") >= limit) # FIX: Remove stale\n",
    "df_clean.display()\n",
    "\n",
    "# 2. Volume Test (Expectation: Min 5 rows)\n",
    "if df_clean.count() < 5:\n",
    "    print(f\"Check 2 (Volume): ALERT! Low data volume detected.\")\n",
    "\n",
    "# 3. NULL Values Check\n",
    "nulls = df_clean.filter(F.col(\"cust_id\").isNull())\n",
    "print(f\"Check 3 (NULLs): {nulls.count()} rows with missing Customer IDs.\")\n",
    "df_clean = df_clean.dropna(subset=[\"cust_id\"]) # FIX: Drop critical nulls\n",
    "df_clean.display()\n",
    "\n",
    "# 4. Numeric Distribution (Outlier Detection)\n",
    "outliers = df_clean.filter((F.col(\"amount\") <= 0) | (F.col(\"amount\") > 1000))\n",
    "print(f\"Check 4 (Numeric): {outliers.count()} outliers found.\")\n",
    "df_clean = df_clean.filter(F.col(\"amount\") > 0) # FIX: Filter logical errors\n",
    "df_clean.display()\n",
    "\n",
    "# 5. Uniqueness (Duplicate Check)\n",
    "dupes = df_clean.groupBy(\"order_id\").count().filter(\"count > 1\")\n",
    "print(f\"Check 5 (Uniqueness): {dupes.count()} duplicate IDs found.\")\n",
    "from pyspark.sql import Window\n",
    "df_clean = df_clean.withColumn(\"rn\", F.row_number().over(\n",
    "    Window.partitionBy(\"order_id\").orderBy(F.col(\"updated_at\").desc()))\n",
    ").filter(F.col(\"rn\") == 1).drop(\"rn\") # FIX: De-duplicate, keep latest by updated_at\n",
    "df_clean.display()\n",
    "\n",
    "# 6. Referential Integrity (Join Check)\n",
    "# Find orders with no matching parent in df_parents\n",
    "orphans = df_clean.join(df_parents, \"cust_id\", \"left_anti\")\n",
    "print(f\"Check 6 (Integrity): {orphans.count()} orphaned records found.\")\n",
    "df_clean = df_clean.join(df_parents, \"cust_id\", \"inner\") # FIX: Keep only valid relationships\n",
    "df_clean.display()\n",
    "\n",
    "# 7. String Pattern (Regex Validation)\n",
    "email_p = r\"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}$\"\n",
    "bad_strings = df_clean.filter(~F.col(\"email\").rlike(email_p))\n",
    "print(f\"Check 7 (Patterns): {bad_strings.count()} bad email formats.\")\n",
    "df_clean = df_clean.filter(F.col(\"email\").rlike(email_p)) # FIX: Enforce format\n",
    "df_clean.display()\n",
    "\n",
    "# --- FINAL OUTPUT ---\n",
    "print(f\"\\nAudit Complete. Original: {df.count()} rows | Cleaned: {df_clean.count()} rows\")\n",
    "df_clean.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d598a53a-e7a7-45ab-aa3c-d6c06a9eb0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-data-quality-checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
