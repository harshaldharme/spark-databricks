{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f77169d9-d4fc-44ca-add1-11c3bb02db45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Customized schema Evolution feature in plain spark which should work in GCP unlike databricks (cloudFiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e9221f4-4646-4d89-8a4d-7edaf73746b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, to_json, struct\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SchemaEvolutionGCS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# GCS path\n",
    "gcs_path = \"gs://your-bucket/path/*.csv\"  # or *.json\n",
    "\n",
    "# Expected schema (start with known columns)\n",
    "expected_schema = StructType([])  # Empty means infer first batch\n",
    "\n",
    "# Function to rescue unexpected fields\n",
    "def rescue_unexpected_fields(df, expected_cols):\n",
    "    actual_cols = set(df.columns)\n",
    "    extra_cols = actual_cols - expected_cols\n",
    "    if extra_cols:\n",
    "        df = df.withColumn(\"_rescued_data\", to_json(struct(*[col(c) for c in extra_cols])))\n",
    "        df = df.drop(*extra_cols)\n",
    "    return df\n",
    "\n",
    "# Read files and infer schema dynamically\n",
    "df = spark.read.option(\"header\", \"true\").csv(gcs_path)  # For JSON use .json(gcs_path)\n",
    "\n",
    "# If expected schema is empty, initialize it\n",
    "if len(expected_schema.fields) == 0:\n",
    "    expected_schema = df.schema\n",
    "\n",
    "# Align columns: add missing columns with nulls\n",
    "expected_cols = set(expected_schema.fieldNames())\n",
    "actual_cols = set(df.columns)\n",
    "\n",
    "missing_cols = expected_cols - actual_cols\n",
    "for col_name in missing_cols:\n",
    "    df = df.withColumn(col_name, lit(None))\n",
    "\n",
    "# Rescue unexpected fields\n",
    "df = rescue_unexpected_fields(df, expected_cols)\n",
    "\n",
    "# Reorder columns to match expected schema\n",
    "df = df.select(*expected_schema.fieldNames(), \"_rescued_data\")\n",
    "\n",
    "# Write cleaned data back to GCS or Delta\n",
    "df.write.mode(\"overwrite\").parquet(\"gs://your-bucket/cleaned-data/\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-schema-evolution-gcp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
